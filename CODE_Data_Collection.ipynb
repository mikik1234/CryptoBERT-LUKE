{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data extraction procedures"
      ],
      "metadata": {
        "id": "302nNHHuemg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StockTwits"
      ],
      "metadata": {
        "id": "5kaya0MGgPb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We adapt the methods found in https://github.com/p-hiroshige/stockTwitsAPI, to extract the stocktwits data."
      ],
      "metadata": {
        "id": "TMkuP57mf5iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "sc = Collector()\n",
        "\n",
        "# save the messages on files splitted per chunk from a date to max ID\n",
        "tickers = ['CRO.X', 'SOL.X', 'ALGO.X']\n",
        "chunk = sc.save_history({'symbols': tickers[0], 'start': '2021-11-01T00:00:00Z',  'chunk': 'month'})"
      ],
      "metadata": {
        "id": "3G99gKpkeqss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the monthly data from json format, into a single dictionary of pandas DataFrames.\n",
        "stocktwits_monthly = {}\n",
        "\n",
        "with open('history.20220601.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_06'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "\n",
        "\n",
        "with open('history.20220501.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_05'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "with open('history.20220401.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_04'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "\n",
        "\n",
        "with open('history.20220301.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_03'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('history.20220201.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_02'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('history.20220101.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2022_01'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('history.20211201.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2021_12'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]\n",
        "\n",
        "with open('history.20211101.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "df = pd.json_normalize(data,\n",
        "                       meta=[\n",
        "        'id', 'body', 'created_at',\n",
        "        ['user', 'id'],\n",
        "        ['user', 'username'],\n",
        "        ['entities', 'sentiment', 'basic']\n",
        "    ]\n",
        ")\n",
        "stocktwits_monthly['2021_11'] = df[['id', 'body', 'created_at', 'user.username', 'entities.sentiment.basic']]"
      ],
      "metadata": {
        "id": "41yi_SbYfMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dictionary to excel, with one dataframe per sheet\n",
        "import xlsxwriter\n",
        "fn_save = path + 'StockTwits_CRO_M.xlsx'\n",
        "with pd.ExcelWriter(fn_save) as writer:\n",
        "  for df_name, df in stocktwits_monthly.items():\n",
        "    print(df_name)\n",
        "    df.to_excel(writer, sheet_name=df_name, engine = \"xlsxwriter\")"
      ],
      "metadata": {
        "id": "48_lELwUfdyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit"
      ],
      "metadata": {
        "id": "0_hUp985gSxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract data from reddit, we use the reddit data collector API, sourced from https://github.com/nicovandenhooff/reddit-data-collector"
      ],
      "metadata": {
        "id": "vV1SnIH3gxzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install reddit-data-collector"
      ],
      "metadata": {
        "id": "e2D5-DZghFlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import praw\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "SKeI8h2ThEJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Personal information to be filled by user\n",
        "personal_script = 'personal-script'\n",
        "secret_reddit = 'secret-reddit'\n",
        "username = 'user-name'\n",
        "pw = 'pass-word'\n",
        "user_agent = 'MyAPI/0.0.1'"
      ],
      "metadata": {
        "id": "XgIvBgCWhE3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import reddit_data_collector as rdc\n",
        "data_collector = rdc.DataCollector(\n",
        "    client_id=personal_script,\n",
        "    client_secret=secret_reddit,\n",
        "    user_agent=user_agent,\n",
        "    username=username,\n",
        "    password=pw\n",
        ")"
      ],
      "metadata": {
        "id": "yyOAROrlhRiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts, comments = data_collector.get_data(\n",
        "    subreddits=[\"WallStreetBetsCrypto\", \"CryptoMoonShots\", \"CryptoCurrency\", \"Bitcoin\", \"SHIBArmy\", \"Shibainucoin\", \"ethtrader\"],\n",
        "    post_filter=\"top\",\n",
        "    top_post_filter = \"year\",\n",
        "    comment_data=True,\n",
        "    replies_data=True,\n",
        "    replace_more_limit=16,\n",
        "    dataframe=True\n",
        ")"
      ],
      "metadata": {
        "id": "2zFFEbZUhTdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comments.shape)"
      ],
      "metadata": {
        "id": "vd6OY_hHhYRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reddit_posts = posts\n",
        "df_reddit_comments = comments\n",
        "with pd.ExcelWriter('Reddit_Corpora.xlsx') as writer:  \n",
        "    df_reddit_posts.to_excel(writer, sheet_name='Reddit_Posts')\n",
        "    df_reddit_comments.to_excel(writer, sheet_name='Reddit_Comments')"
      ],
      "metadata": {
        "id": "n7_NbRxbhhPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The following classes are used for extracting stocktwits data "
      ],
      "metadata": {
        "id": "8WAJz77YeK7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "class twitStreamer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.url = \"https://api.stocktwits.com/api/2/\"\n",
        "        self.headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "    def get_user_msgs(self, user_id, since=0, max=0, limit=0, callback=None, filter=None):\n",
        "\n",
        "        \"\"\"Returns the most recent 30 messages for the specified user.\n",
        "\n",
        "        Args:\n",
        "            user_id (int) = User ID or Username of the stream's user\n",
        "                            you want to show (Required)\n",
        "            since (int) = Returns results with an ID greater than (\n",
        "                          more recent than) the specified ID.\n",
        "            max (int) = Returns results with an ID less than\n",
        "                        (older than) or equal to the specified ID.\n",
        "            limit (int) = Default and max limit is 30.\n",
        "                          This limit must be a number under 30.\n",
        "            callback = Define your own callback function name,\n",
        "                       add this parameter as the value.\n",
        "            filter (string) = Filter messages by links, charts, or videos.\n",
        "                              (Optional)\n",
        "\n",
        "        Return:\n",
        "            raw_json (dict) = The JSON output unparsed\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        url = self.url + 'streams/user/' + user_id + '.json'\n",
        "\n",
        "        data = {\n",
        "                 'since': '{}'.format(since),\n",
        "                 'max': '{}'.format(max),\n",
        "                 'limit': '{}'.format(limit),\n",
        "                 # Fix when you figure out what this is\n",
        "                 # 'callback' : '{}'.format(None),\n",
        "                 'filter': '{}'.format(filter)\n",
        "                }\n",
        "\n",
        "        r = requests.get(url, headers=self.headers, params=data)\n",
        "        if r.status_code != 200:\n",
        "            raise Exception('Unable to Return Request {}'\n",
        "                            .format(r.status_code))\n",
        "\n",
        "        raw_json = r.json()\n",
        "        return raw_json\n",
        "\n",
        "    def get_symbol_msgs(self, symbol_id, since=0, max=0, limit=0, callback=None, filter=None):\n",
        "\n",
        "        '''Returns the most recent 30 messages for the specified symbol.\n",
        "\n",
        "        Args:\n",
        "            symbol_id:\tTicker symbol, Stock ID, or\n",
        "                        RIC code of the symbol (Required)\n",
        "            since:\tReturns results with an ID greater than (more recent than)\n",
        "                    the specified ID.\n",
        "            max:\tReturns results with an ID less than (older than) or\n",
        "                    equal to the specified ID.\n",
        "            limit:\tDefault and max limit is 30. This limit must be a\n",
        "                    number under 30.\n",
        "            callback:\tDefine your own callback function name,\n",
        "                        add this parameter as the value.\n",
        "            filter:\tFilter messages by links, charts, videos,\n",
        "                    or top. (Optional)\n",
        "\n",
        "        Return:\n",
        "            raw_json (dict) = The JSON output unparsed\n",
        "\n",
        "        '''\n",
        "\n",
        "        url = self.url + 'streams/symbol/' + symbol_id + '.json'\n",
        "        print(\"url to get msgs:\" +url)\n",
        "        data = {\n",
        "                 'since': '{}'.format(since),\n",
        "                 'max': '{}'.format(max),\n",
        "                 'limit': '{}'.format(limit),\n",
        "                 # Fix when you figure out what this is\n",
        "                 # 'callback' : '{}'.format(None),\n",
        "                 'filter': '{}'.format(filter)\n",
        "                }\n",
        "\n",
        "        r = requests.get(url, headers=self.headers, params=data)\n",
        "        if r.status_code != 200:\n",
        "            raise Exception('Unable to Return Request {}'\n",
        "                            .format(r.status_code))\n",
        "\n",
        "        raw_json = r.json()\n",
        "        return raw_json\n",
        "\n",
        "    def get_specified_conversation_msgs(self, conversation_id, since=0, max=0, limit=0, callback=None):\n",
        "\n",
        "        '''\n",
        "\n",
        "        Args:\n",
        "            conversation_id:\tThe message ID of the parent message\n",
        "                                to a conversation. (Required)\n",
        "            since:\tReturns results with an ID greater than (more recent than)\n",
        "                    the specified ID.\n",
        "            max:\tReturns results with an ID less than (older than) or equal\n",
        "                    to the specified ID.\n",
        "            limit:\tDefault and max limit is 30. This limit must be a\n",
        "                    number under 30.\n",
        "            callback:\tDefine your own callback function name, add this\n",
        "                        parameter as the value.\n",
        "\n",
        "        Return:\n",
        "            raw_json (dict) = The JSON output unparsed\n",
        "\n",
        "        '''\n",
        "\n",
        "        url = self.url + 'streams/conversation/' + conversation_id + '.json'\n",
        "\n",
        "        data = {\n",
        "                 'since': '{}'.format(since),\n",
        "                 'max': '{}'.format(max),\n",
        "                 'limit': '{}'.format(limit)\n",
        "                 # Fix when you figure out what this is\n",
        "                 # 'callback' : '{}'.format(None),\n",
        "                }\n",
        "\n",
        "        r = requests.get(url, headers=self.headers, params=data)\n",
        "        if r.status_code != 200:\n",
        "            raise Exception('Unable to Return Request {}'\n",
        "                            .format(r.status_code))\n",
        "\n",
        "        raw_json = r.json()\n",
        "        return raw_json\n"
      ],
      "metadata": {
        "id": "90GhIVm8ekoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The class for collecting twits of Stocktwits\n",
        "\n",
        "    A collection of methods to simplify your downloading\n",
        "\"\"\"\n",
        "import sys\n",
        "import warnings\n",
        "from contextlib import contextmanager\n",
        "from io import StringIO\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "#import stockTwitFetchAPI.stocktwitapi as st\n",
        "\n",
        "class Collector():\n",
        "    ts = None\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        the core of API is the package stockTwitFetchAPI with the class twitStreamer\n",
        "        \"\"\"\n",
        "        self.ts = twitStreamer()\n",
        "\n",
        "    @contextmanager\n",
        "    def hold_output(self):\n",
        "        \"\"\"\n",
        "        hold output\n",
        "\n",
        "        This method is temporary until PR approval:\n",
        "        https://github.com/p-hiroshige/stockTwitsAPI/pull/1\n",
        "\n",
        "            Example:\n",
        "                with hold_output() as (out, err):\n",
        "                    method_with_a_print()\n",
        "                captured_output = out.getvalue().strip()\n",
        "\n",
        "        \"\"\"\n",
        "        new_out, new_err = StringIO(), StringIO()\n",
        "        old_out, old_err = sys.stdout, sys.stderr\n",
        "        try:\n",
        "            sys.stdout, sys.stderr = new_out, new_err\n",
        "            yield sys.stdout, sys.stderr\n",
        "        finally:\n",
        "            sys.stdout, sys.stderr = old_out, old_err\n",
        "\n",
        "    def there_is_symbol(self, symbols_fetched, symbols_target):\n",
        "        \"\"\"\n",
        "        check if in the message there are the symbols target\n",
        "\n",
        "            Arguments:\n",
        "                :symbols_fetched (list of dict): list of symbols\n",
        "                :symbols_target (list of string): list of symbols names\n",
        "            Returns:\n",
        "                a boolean, True if there is at least one symbol of target in the symbols fetched\n",
        "        \"\"\"\n",
        "        for symbol in symbols_fetched:\n",
        "            if symbol[\"symbol\"] in symbols_target:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def clean_data(self, messages, event):\n",
        "        \"\"\"\n",
        "        clean data\n",
        "\n",
        "            Arguments:\n",
        "                :messages (list of dict): list of messages\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "                    symbols (list of str): names of symbols to fetch\n",
        "                    users (list of str): names of users to fetch\n",
        "                    only_combo (bool): if True, fetches only messages of those symbols posted from those users\n",
        "            Returns:\n",
        "                list of unique dictionaries cleaned\n",
        "        \"\"\"\n",
        "        # unique messages\n",
        "        messages = list({ message[\"id\"] : message for message in messages }.values())\n",
        "        if \"only_combo\" in event and event[\"only_combo\"] == True:\n",
        "            if \"symbols\" in event and \"users\" in event:\n",
        "                messages = list({ message[\"id\"] : message for message in messages if self.there_is_symbol(message[\"symbols\"], event[\"symbols\"]) and message[\"user\"][\"username\"] in event[\"users\"] }.values())\n",
        "        return messages\n",
        "\n",
        "    def get_data(self, event):\n",
        "        \"\"\"\n",
        "        get data from Stocktwits, default last 30 messages\n",
        "\n",
        "            Arguments:\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "                    symbols (list of str): names of symbols to fetch\n",
        "                    users (list of str): names of users to fetch\n",
        "                    min (int): optional, min ID\n",
        "                    max (int): optional, max ID\n",
        "                    limit (int): optional, defalt 30 messages\n",
        "            Returns:\n",
        "                list of messages\n",
        "        \"\"\"\n",
        "        messages = []\n",
        "\n",
        "        if \"min\" not in event:\n",
        "            event[\"min\"] = 0\n",
        "\n",
        "        if \"max\" not in event:\n",
        "            event[\"max\"] = 0\n",
        "\n",
        "        if \"limit\" not in event:\n",
        "            event[\"limit\"] = 30\n",
        "\n",
        "        if \"users\" in event:\n",
        "            for user in event[\"users\"]:\n",
        "                response = self.ts.get_user_msgs(user_id=user, since=event[\"min\"], max=event[\"max\"], limit=event[\"limit\"], callback=None, filter=None)\n",
        "                messages.extend(response[\"messages\"])\n",
        "\n",
        "        if \"symbols\" in event:\n",
        "            for symbol in event[\"symbols\"]:\n",
        "                with self.hold_output() as (out, err):\n",
        "                    try:\n",
        "                        response = self.ts.get_symbol_msgs(symbol_id=symbol, since=event[\"min\"], max=event[\"max\"], limit=event[\"limit\"], callback=None, filter=None)\n",
        "                    except Exception as error:\n",
        "                        print(event)\n",
        "                        raise Exception(error)\n",
        "                    messages.extend(response[\"messages\"])\n",
        "\n",
        "        return self.clean_data(messages, event)\n",
        "\n",
        "    def is_younger(self, first_date, second_date):\n",
        "        \"\"\"\n",
        "        compare a date with a second date\n",
        "\n",
        "            Argument:\n",
        "                :first_date (str): datetime with format %Y-%m-%dT%H:%M:%SZ \n",
        "                :second_date (str): another date with format %Y-%m-%dT%H:%M:%SZ\n",
        "            Returns:\n",
        "                a boolean, True if first date is younger than second one\n",
        "        \"\"\"\n",
        "        first = datetime.strptime(first_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        second = datetime.strptime(second_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "        if first <= second:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_same_chunk(self, first_date, second_date, chunk = \"day\"):\n",
        "        \"\"\"\n",
        "        compare a date with a second date\n",
        "\n",
        "            Argument:\n",
        "                :first_date (str): datetime with format %Y-%m-%dT%H:%M:%SZ \n",
        "                :second_date (str): another date with format %Y-%m-%dT%H:%M:%SZ\n",
        "                :chunk (str): day, week or month, default day\n",
        "            Returns:\n",
        "                a boolean, True if the dates are of the same chunk\n",
        "        \"\"\"\n",
        "        first = datetime.strptime(first_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        second = datetime.strptime(second_date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        is_same = False\n",
        "\n",
        "        if chunk == \"day\":\n",
        "            is_same = first.strftime(\"%Y-%m-%d\") == second.strftime(\"%Y-%m-%d\")\n",
        "        if chunk == \"week\":\n",
        "            is_same = first.strftime(\"%W\") == second.strftime(\"%W\")\n",
        "        if chunk == \"month\":\n",
        "            is_same = first.replace(day=1).strftime(\"%Y-%m-%d\") == second.replace(day=1).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        return is_same\n",
        "\n",
        "    def get_cursor(self, messages):\n",
        "        \"\"\"\n",
        "        get cursor with oldest date, min ID and max ID\n",
        "\n",
        "            Arguments:\n",
        "                :messages (list[dict]): list of messages\n",
        "            Returns:\n",
        "                a dictionary with oldest_date, min ID, earliest_date and max (ID)\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"oldest_date\": messages[-1][\"created_at\"],\n",
        "            \"min\": messages[-1][\"id\"],\n",
        "            \"earliest_date\": messages[0][\"created_at\"],\n",
        "            \"max\": messages[0][\"id\"]\n",
        "        }\n",
        "\n",
        "    def clean_history(self, cursor, history, chunk = \"day\"):\n",
        "        \"\"\"\n",
        "        clean history from messages with different chunk\n",
        "\n",
        "            Arguments:\n",
        "                :cursor (dict): dictionary with the keys oldest_date, min ID, earliest_date and max (ID)\n",
        "                :history (list[dict]): list of messages\n",
        "                :chunk (str): day, week or month, default day\n",
        "            Returns:\n",
        "                history cleaned\n",
        "        \"\"\"\n",
        "        history_length = 0\n",
        "        same_oldest_date = 0\n",
        "        same_earliest_date = 0\n",
        "\n",
        "        for message in history:\n",
        "            history_length += 1\n",
        "            if self.is_same_chunk(message[\"created_at\"], cursor[\"oldest_date\"], chunk):\n",
        "                same_oldest_date += 1\n",
        "            if self.is_same_chunk(message[\"created_at\"], cursor[\"earliest_date\"], chunk):\n",
        "                same_earliest_date += 1\n",
        "\n",
        "        if history_length == (same_oldest_date + same_earliest_date):\n",
        "            current_chunk = cursor[\"oldest_date\"]\n",
        "            indexes_to_delete = []\n",
        "            for index, message in enumerate(history):\n",
        "                if self.is_same_chunk(message[\"created_at\"], current_chunk):\n",
        "                    indexes_to_delete.append(index)\n",
        "            for index in reversed(indexes_to_delete):\n",
        "                del history[index]\n",
        "        elif not history_length == same_oldest_date and not history_length == same_earliest_date:\n",
        "            warnings.warn(f\"method clean_history, messages amount: {history_length}, {same_oldest_date} messages of {cursor['oldest_date']} and {same_earliest_date} messages of {cursor['earliest_date']}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def walk(self, event, cursor, history):\n",
        "        \"\"\"\n",
        "        walk along the messages like a shrimp \n",
        "\n",
        "            Arguments:\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "                :cursor (dict): dictionary with the keys oldest_date, min ID, earliest_date and max (ID)\n",
        "                :history (list[dict]): list of messages\n",
        "            Returns:\n",
        "                cursor, history\n",
        "        \"\"\"\n",
        "        event[\"max\"] = cursor[\"min\"]\n",
        "        messages = self.get_data(event)\n",
        "        cursor = self.get_cursor(messages)\n",
        "        chunk = event[\"chunk\"] if \"chunk\" in event else \"day\"\n",
        "\n",
        "        if not self.is_same_chunk(cursor[\"oldest_date\"], cursor[\"earliest_date\"], chunk):\n",
        "            messages = self.clean_history(cursor, messages, chunk)\n",
        "            cursor = self.get_cursor(messages)\n",
        "        history.extend(messages)\n",
        "\n",
        "        return cursor, history\n",
        "\n",
        "    def get_history(self, event):\n",
        "        \"\"\"\n",
        "        get history from Stocktwist, default last 30 messages\n",
        "\n",
        "            Arguments:\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "                    start (datetime): optional, min datetime\n",
        "                    is_verbose (bool): optional, if True comments will be printed\n",
        "            Returns:\n",
        "                list of messages\n",
        "        \"\"\"\n",
        "        history = []\n",
        "        messages = self.get_data(event)\n",
        "        history.extend(messages)\n",
        "\n",
        "        if \"start\" in event:\n",
        "            cursor = self.get_cursor(messages)\n",
        "            while self.is_younger(event[\"start\"], cursor[\"oldest_date\"]) and not event[\"start\"] == cursor[\"oldest_date\"]:\n",
        "                if \"is_verbose\" in event and event[\"is_verbose\"] is True:\n",
        "                    print(f\"method get_history, start: {event['start']}, cursor: {cursor['oldest_date']}\")\n",
        "                cursor, history = self.walk(event, cursor, history)\n",
        "        # elif \"min\" in event and event[\"min\"] > 0:\n",
        "        #     cursor = self.get_cursor(messages)\n",
        "        #     while event[\"min\"] < cursor[\"min\"]:\n",
        "        #         cursor, history = self.walk(event, cursor, history)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def get_date(self, chunk = \"day\", date = None, jump_chunk = False):\n",
        "        \"\"\"\n",
        "        get date at midnight about chunk\n",
        "\n",
        "            Arguments:\n",
        "                :chunk (str): day, week or month, default day\n",
        "                :date (str): datetime with format %Y-%m-%dT%H:%M:%SZ\n",
        "                :jump_chunk (bool): True if you want to jump one chunk\n",
        "            Returns:\n",
        "                string of date at midnight about that chunk or next one\n",
        "        \"\"\"\n",
        "        if date is None:\n",
        "            current = datetime.now()\n",
        "        else:\n",
        "            current = datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        jump = 1\n",
        "        start = current\n",
        "        if chunk == \"week\":\n",
        "            jump = 7\n",
        "            start = current - timedelta(days=current.weekday())\n",
        "        elif chunk == \"month\":\n",
        "            jump = 0\n",
        "            start = current.replace(day=1)\n",
        "        if jump_chunk == True:\n",
        "            start = start - timedelta(days=jump)\n",
        "            if chunk == \"month\":\n",
        "                month = start.month - 1\n",
        "                year = start.year\n",
        "                if month == 0:\n",
        "                    month = 12\n",
        "                    year = start.year - 1\n",
        "                start = start.replace(month=month, year=year)\n",
        "        return start.strftime(\"%Y-%m-%dT00:00:00Z\")\n",
        "\n",
        "    def update_event(self, key, value, event):\n",
        "        \"\"\"\n",
        "        update a specific key of event\n",
        "\n",
        "            Arguments:\n",
        "                :key (str): attribute name of event\n",
        "                :value (mix): value you want to replace on that key\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "            Returns:\n",
        "                dictionary with the attribute named key changed with value\n",
        "        \"\"\"\n",
        "        chunk = {}\n",
        "        for k in event.keys():\n",
        "            chunk[k] = event[k]\n",
        "        chunk[key] = value\n",
        "        return chunk\n",
        "\n",
        "    def get_temporary_event(self, messages, current_chunk, event):\n",
        "        \"\"\"\n",
        "        get temporary chunk event from messages\n",
        "\n",
        "            Arguments:\n",
        "                :messages (list[dict]): list of messages\n",
        "                :current_chunk (dict): dictionary fully described in save_history()\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "            Returns:\n",
        "                the temporary chunk event updated with the partial start and new min\n",
        "        \"\"\"\n",
        "        cursor = self.get_cursor(messages)\n",
        "        oldest_date = self.get_date(event[\"chunk\"], cursor[\"oldest_date\"])\n",
        "        if event[\"start\"] == oldest_date:\n",
        "            oldest_date = self.get_date(event[\"chunk\"], oldest_date, True)\n",
        "        next_chunk = self.update_event(\"start\", oldest_date, current_chunk)\n",
        "        if self.is_younger(next_chunk[\"start\"], event[\"start\"]):\n",
        "            if event[\"start\"] == current_chunk[\"start\"]:\n",
        "                return next_chunk\n",
        "            else:\n",
        "                next_chunk[\"start\"] = event[\"start\"]\n",
        "        next_chunk[\"max\"] = cursor[\"min\"]\n",
        "        return next_chunk\n",
        "\n",
        "    def get_file_name(self, history, current_chunk, event):\n",
        "        \"\"\"\n",
        "        get filename\n",
        "\n",
        "           Arguments:\n",
        "                :history (list[dict]): list of messages\n",
        "                :current_chunk (dict): dictionary like event\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "            Returns:\n",
        "                the file name\n",
        "        \"\"\"\n",
        "        chunk = datetime.strptime(current_chunk[\"start\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        next_chunk = self.get_temporary_event(history, current_chunk, event)\n",
        "        cursor = self.get_cursor(history)\n",
        "        if self.get_date(event[\"chunk\"], next_chunk[\"start\"]) == self.get_date(event[\"chunk\"], cursor[\"earliest_date\"]):\n",
        "            chunk = datetime.strptime(next_chunk[\"start\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        return f'{event[\"filename_prefix\"]}{chunk.strftime(\"%Y%m%d\")}{event[\"filename_suffix\"]}'\n",
        "\n",
        "    def save_data(self, history, current_chunk, event):\n",
        "        \"\"\"\n",
        "        save data\n",
        "\n",
        "            Arguments:\n",
        "                :history (list[dict]): list of messages\n",
        "                :current_chunk (dict): dictionary like event\n",
        "                :event (dict): dictionary fully described in save_history()\n",
        "            Returns:\n",
        "                the temporary chunk event updated with the partial start and new max\n",
        "        \"\"\"\n",
        "        filename = self.get_file_name(history, current_chunk, event)\n",
        "        next_chunk = self.get_temporary_event(history, current_chunk, event)\n",
        "        cursor = self.get_cursor(history)\n",
        "        if not self.is_same_chunk(cursor[\"oldest_date\"], cursor[\"earliest_date\"], event[\"chunk\"]):\n",
        "            history = self.clean_history(cursor, history, event[\"chunk\"])\n",
        "            cursor = self.get_cursor(history)\n",
        "            next_chunk[\"max\"] = cursor[\"min\"]\n",
        "        with open(filename, \"w\") as fh:\n",
        "            json.dump(history, fh)\n",
        "        return next_chunk\n",
        "\n",
        "    def save_history(self, event):\n",
        "        \"\"\"\n",
        "        save history from Stocktwist on files splitted by chunk per day, week or month\n",
        "\n",
        "            Arguments:\n",
        "                :event (dict):\n",
        "                    symbols (list[str]): names of symbols to fetch\n",
        "                    users (list[str]): names of users to fetch\n",
        "                    only_combo (bool): optional, if True, fetches only messages of those symbols posted from those users\n",
        "                    min (int): optional, min ID\n",
        "                    max (int): optional, max ID\n",
        "                    limit (int): optional, default 30 messages\n",
        "                    start (str): optional, min datetime\n",
        "                    chunk (str): optional (day, week or month), default day\n",
        "                    filename_prefix (str): optional, default \"history.\"\n",
        "                    filename_suffix (str): optional, default \".json\"\n",
        "                    is_verbose (bool): optional, if True comments will be printed\n",
        "            Returns:\n",
        "                last temporary chunk event discarded\n",
        "        \"\"\"\n",
        "        history = []\n",
        "        if \"chunk\" not in event:\n",
        "            event[\"chunk\"] = \"day\"\n",
        "\n",
        "        if \"start\" not in event:\n",
        "            event[\"start\"] = self.get_date(event[\"chunk\"])\n",
        "\n",
        "        if \"filename_prefix\" not in event:\n",
        "            event[\"filename_prefix\"] = \"history.\"\n",
        "\n",
        "        if \"filename_suffix\" not in event:\n",
        "            event[\"filename_suffix\"] = \".json\"\n",
        "\n",
        "        messages = self.get_data(event)\n",
        "        history.extend(messages)\n",
        "        cursor = self.get_cursor(messages)\n",
        "        oldest_date = self.get_date(event[\"chunk\"], cursor[\"oldest_date\"])\n",
        "        chunk = self.update_event(\"start\", oldest_date, event)\n",
        "\n",
        "        if \"start\" in event:\n",
        "            if \"is_verbose\" in event and event[\"is_verbose\"] is True:\n",
        "                print(f\"method save_history, start: {event['start']}, cursor: {cursor['oldest_date']}, next chunk: {chunk['start']}\")\n",
        "            while self.is_younger(event[\"start\"], chunk[\"start\"]):\n",
        "                history = self.get_history(chunk)\n",
        "                chunk = self.save_data(history, chunk, event)\n",
        "                if \"is_verbose\" in event and event[\"is_verbose\"] is True:\n",
        "                    print(f\"method save_history, start: {event['start']}, cursor: {cursor['oldest_date']}, next chunk: {chunk['start']}\")\n",
        "                history = []\n",
        "        # elif \"min\" in event and event[\"min\"] > 0:\n",
        "        #     while event[\"min\"] < cursor[\"min\"]:\n",
        "        #         history = self.get_history(chunk)\n",
        "        #         chunk = self.save_data(history, chunk, event)\n",
        "        #         history = []\n",
        "\n",
        "        return chunk"
      ],
      "metadata": {
        "id": "6PEsS-rpeZMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "3p2vKEG_h1y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The cleaning function is defined as follows:\n",
        "import regex\n",
        "def remove_wallets(text): \n",
        "  # print(text)\n",
        "  return ' '.join(word for word in str(text).split() if len(word)<40)\n",
        "def clean_df(df):\n",
        "  df = df.drop_duplicates()\n",
        "  df = df[df.notnull()]\n",
        "  df = df.replace(r'[\\u4e00-\\u9fff]+', '', regex = True) # Remove all common asian letters\n",
        "  df = df.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "  df = df.replace(r'[@][A-Za-z0-9_]+', '', regex=True).replace(r'[#][A-Za-z0-9_]+', '', regex=True).replace(r'[$][A-Za-z0-9_ ]+', '', regex=True).replace(r'[/][A-Za-z0-9_ ]+', '', regex=True)\n",
        "  df = df.replace(r'RT : ', '', regex=True)\n",
        "  df = df.replace(r'&amp;', 'and', regex=True).replace(r'&amp', 'and', regex=True)\n",
        "  df = df.replace(r'â€™', '\\'', regex=True).replace(r'&#39;', '\\'', regex=True).replace(r'&#x200B;', '', regex=True).replace(r'&;', '\\'', regex=True)\n",
        "  df = df.replace(r'\\.X', '', regex=True).replace(r'\\.x', '', regex=True)\n",
        "  df = df.replace(r'  ', ' ', regex=True).replace(r'   ', ' ', regex=True).replace(r'    ', ' ', regex=True)\n",
        "  df = df.replace(r'@', '', regex=True)\n",
        "  df = df.replace(r' \\| ', '', regex = True).replace(r'\\|', '', regex = True)\n",
        "  df = df.replace(r'\\.\\.+', \"...\", regex=True)\n",
        "  df = df.str.lower()\n",
        "  df = df.replace(r'&quot;', '', regex=True)\n",
        "  df = df.apply(lambda x: remove_wallets(x))\n",
        "  df = df.drop_duplicates()\n",
        "  df = df[df.str.split().str.len().ge(4)] # Remove all the posts shorter than 4 words \n",
        "  return df"
      ],
      "metadata": {
        "id": "vkx9MmKqh4ht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}